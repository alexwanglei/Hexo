title: RDD论文阅读
date: 2015-12-18 10:26:13
categories: Spark
tags: [RDD, Spark]
---

论文*[Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing](http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf)* 出自UC Berkeley实验室Spark项目的作者，是Spark中重要概念RDD的研究论文。
<!-- more -->
## 为什么提出RDD？

现有的集群计算框架(如MapReduce)都提供访问集群计算资源的抽象，但缺少利用分布式内存的抽象。这使得它们不能高效的适用于一类重要的新兴应用，这类应用的特点是复用跨多个计算过程的中间结果。数据复用有两种典型的应用场景：
1. 迭代机器学习和图算法；
2. 交互式数据挖掘，在相同的数据子集上运行多次即席查询(ad-hoc query)；

当前的大多数框架通过将数据写入外部存储系统的方式实现计算间的数据复用。数据复制，磁盘I/O和序列化需要大量时间开销，主导了应用的执行时间。因此作者提出一种称为**resilient distributed datasets(RDDs)**的抽象，在广泛的应用中能有效实现数据复用。RDDs是容错的、并行数据结构，能让用户显式的将计算中间结果保持在内存中；控制它们的分区以优化数据位置；使用丰富的操作符操作它们。设计RDDs的主要挑战是定义一个能够提供有效容错的编程接口。RDDs提供了基于粗粒度转换的接口(例如：map、filter、join)，这些转换将相同的操作应用到许多数据项上。这也体现了RDDs的两个显著特点：
1. **并行**——转换将相同的操作应用到许多数据条目上。
2. **容错**——通过记录用在数据集上的转换(数据沿袭)实现容错。

如果RDD的一个分区丢失，RDD有足够的信息知道它是如何从其他RDD推导过来的，从而可以重新计算得到该分区。因此丢失的数据能被恢复，而且通常很快，不需要高昂的数据复制操作。

## 弹性分布式数据集(RDDs)
一个RDD是一个只读的，分区的记录集合。RDDs只能通过对(1)稳定存储上的数据或(2)其他RDDs的确定操作来创建。RDDs不需要在所有时间都是具体存在的。因为一个RDD有足够的信息知道它如何从其他的数据集推导出来(它的沿袭)，最终从稳定存储上的数据计算出它的分区。这是一个强大的特性：本质上，程序不会引用到失效后无法重构的RDD。用户能控制RDDs的两个方面：
1. 持久化(persistence)：指定要复用的RDDs并选择存储策略(如存储在内存)
2. 分区(partitioning)：要求RDDs基于每条记录中的key将元素分配到不同的机器。

RDDs最适合对数据集的全部元素进行相同操作的批处理应用。在这种情形下，RDDs能有效的将每个转换记为沿袭图谱中的一步，并且能恢复丢失的分区而不需要记录大量数据。RDDs不适合对共享状态做异步细粒度更新的应用，如web应用的存储系统，增量web爬虫。RDDs的目标是提供对批处理分析的有效编程模型。

## Spark编程接口
Spark使用Scala实现RDD的API。选择Scala是由于它同时具备简洁(交互使用很方便)和高效(静态类型)的特点。下表中列出了Spark中主要的RDD *transformations*和*actions*操作。*transformations*定义新RDD是惰性操作，*actions*启动计算返回结果给程序或写数据到外部存储。一些操作如*join*只用于key-value偶对类型的RDD。用户还可以对RDD调用*persist*方法，表明要在后续的操作中复用RDD，Spark默认将RDDs保存在内存中， 但是如果RAM不足会溢写到硬盘。
<table><tr><th>Ops</th><th>Funs</th></tr><tr><td style="font-weight:bold">Transformations</td><td style="font-style:italic">map(f:T=>U) : RDD[T]=>RDD[U]<br>filter(f:T=>Bool) : RDD[T]=>RDD[T]<br>flatMap(f:T=>Seq[U]) : RDD[T]=>RDD[U]<br>sample(fraction: Float) : RDD[T]=>RDD[T] (Deterministic sampling)<br>groupByKey() : RDD[(K,V)]=>RDD[(K, Seq[V])]<br>reduceByKey(f:(V,V)=>V) : RDD[(K,V)]=>RDD[(K,V)]<br>union() : (RDD[T]=>RDD[T])=>RDD[T]<br>join() : (RDD[(K,V)],RDD[(K,W)])=>RDD[(K,(V,W))]<br>cogroup() : (RDD[T],RDD[U])=>RDD[(T,U)]<br>crossProduct() : (RDD[T],RDD[U])=>RDD[(T,U)]<br>mapValues(f:V=>W) : RDD[(K,V)]=>RDD[(K,W)] (Preserves partitioning)<br>sort(c:Comparator[K]) : RDD[(K,V)]=>RDD[(K,V)]<br>partitionBy(p:Partitioner[K]) : RDD[(K,V)]=>RDD[(K,V)]</td></tr><tr><td style="font-weight:bold">Actions</td><td style="font-style:italic">count() : RDD[T]=>Long<br>collect() : RDD[T]=>Seq[T]<br>reduce(f:(T,T)=>T) : RDD[T]=>T<br>lookup(k:K) : RDD[(K,V)]=>Seq[V] (On hash/range partitioned RDDs)<br>save(path:String) : Outputs RDD to a storage system, e.g.,HDFS</td></tr></table>

## 如何表示RDDs
提供RDDs抽象的一个挑战是选择一种表示它们的方式，能够在广泛的转换间追踪沿袭。理想情况下，实现RDDs的系统应该提供尽可能丰富的转换操作集合(如上表所示)，并且支持用户以任意方式组合它们。作者提出了一种简单的基于图的RDDs表示方式。简而言之，通过通用接口表示每个RDD，这些通用接口提供五个方面的信息：
 1. 分区集合，是数据集的原子块；
 2. 父RDDs的依赖集合；
 3. 基于父RDD计算数据集的函数；
 4. 分区模式的元数据；
 5. 数据位置；

<table><tr><th>操作</th><th>含义</th></tr><tr><td>partitions()</td><td>返回分区对象列表</td></tr><tr><td>preferredLocations(p)</td><td>根据数据局部性给出最快访问到分区p的节点</td></tr><tr><td>dependencies()</td><td>返回依赖列表</td></tr><tr><td>iterator(p,parentIters)</td><td>迭代父分区计算得到分区p的元素</td></tr><tr><td>partitioner()</td><td>返回指定RDD是否为hash/range分区的元数据</td></tr></table>
在设计上述接口中最重要的一个问题是如何表示RDDs之间的依赖。作者将依赖分成两类：
1. 窄依赖：每个父RDD分区至多被一个子RDD分区使用；
2. 宽依赖：多个子分区会依赖一个父RDD分区；

例如，*map*导致窄依赖，*join*导致宽依赖。这种区分非常有用。第一，窄依赖允许在一个集群节点上流水式执行，它能计算全部的父分区。相反，宽依赖要求来自所有父分区的数据都可用并且使用像MapReduce这样的操作进行跨节点的洗牌。第二，在一个节点失效后，恢复窄依赖的RDD更高效，因为只有丢失的父分区需要被重新计算，并且它们可以在不同的节点并行的计算。相反，在具有宽依赖的沿袭图谱中，单个节点失效会导致丢失一些来自RDD全部祖先的分区，因此要求计算重新执行。一些RDD实现的概述：
**HDFS files:** 输入RDDs是HDFS中的文件。对于这些RDDs，*partitions*返回针对每个文件块的一个分区(带有块的偏移存储在每个分区对象中)；*preferredLocations*给出块所在的节点；*iterator*读取块。
**map:** 在任何RDD上调用*map*返回MappedRDD对象。这个对象有和父RDD相同的分区和首选位置，但是在它的*iterator*方法中将传递给*map*的函数应用到父RDD的记录上。
**union:** 在两个RDDs上调用union返回一个RDD，它的分区是两个父RDD的分区的并。每个子分区是从窄依赖的父分区计算得来的。
**sample:** Sampling与Mapping类似，除了RDD存储了对每个分区到决定的采样父记录随机数生成器种子。
**join:** 连接两个RDDs会导致两个窄依赖(如果它们都是用相同partitioner进行hash/range分区的)，或者两个宽依赖，或一个混合(如果一个父RDD有partitioner但另一个没有)。在其他情况，输出RDD有一个partitioner(从父RDD继承的或默认的hash partitioner)。

![](http://7xpf9w.com1.z0.glb.clouddn.com/RDD%20deps.png)

## RDDs的实现
作者用14000行Scala代码实现了Spark系统。系统运行在Mesos集群管理器上。Spark能从Hadoop输入源(例如HDFS、HBase)读取数据。下面简要介绍系统的几个部分。
#### 任务调度
Spark调度器使用上一节描述的RDDs的表示。当用户对一个RDD运行action操作(如*count*或*save*)时，调度器检查RDD的沿袭图谱，然后建立*stages*的DAG图来执行。每个stage包含尽可能多的窄依赖流水式转换。stages的边界是需要宽依赖的shuffle操作，或任何已经计算出的分区，这样可以省去从父RDD开始的计算。调度器启动任务从每个stage来计算缺失的分区直到计算出目标RDD。

![](http://7xpf9w.com1.z0.glb.clouddn.com/stagesDAG.png)

调度器基于数据的局部性分配任务给机器。如果一个任务需要处理的分区在一个节点的内存中，我们把这个任务发送到那个节点。否则，如果任务处理的分区，其对应的RDD提供首选位置(如HDFS文件)，我们就把任务发送到这些首选位置。对于宽依赖(如shuffle依赖)，我们将中间结果具体化到持有父分区的节点上以简化故障恢复，与MapReduce具体化map输出很相似。如果任务运行失败，只要它的stage的父stage仍然可用，就在其他节点上重新运行任务。如果一些stages变得不可用了，我们重新提交任务来计算缺失的分区。尽管可以直接复制RDD的沿袭图谱，但我们还是不能容忍调度器失效。

#### 解释器集成
Scala包含一个类似于Ruby和Python的交互式shell。可以低延迟的获取内存中的数据，能让用户从解释器交互式地运行Spark来查询大数据集。用户在Scala解释器中输入的每行语句被编译成一个类，然后载入到JVM，并在上面调用函数。这个类包含单例对象，单例对象包含语句中的变量或函数，并在初始化方法中运行语句。我们在Spark中对解释器做了两个改变：
1. *Class shipping:* 让worker节点拉取每行语句对应类的字节码，让解释器通过HTTP服务这些类。
2. *Modified code generation:* 正常情况下，每行语句创建的单例对象通过其对应类的静态方法进行访问。这意味着如果要序列化一个闭包时，它引用了前面行中定义的变量，如上例中的Line1.x，Java不会通过对象图追溯来传输具有x的Line1实例。因此worker节点不会收到x。我们修改代码生成逻辑来直接引用每行对象的实例。

下图说明了Spark解释器如何将用户输入的两行代码转换为Java对象。

![](http://7xpf9w.com1.z0.glb.clouddn.com/linesToObjects.png)

#### 内存管理
Spark提供三种可选的RDD持久化存储策略：(1)以反序列化的Java对象形式存储在内存中；(2)以序列化的数据存储在内存中；(3)存储在磁盘上。第一种提供最快的读取性能，因为Java虚拟机可以原生地访问每个RDD元素。第二种让用户在空间受限的时候，以损失较低性能的代价，选择比Java对象图更加存储高效的表示。第三种适用于RDD太大而无法维持在内存中，但每次使用重新计算代价又非常大的情况。为了管理有限可用的内存，在RDDs的级别使用LRU淘汰策略。当计算出一个新RDD分区却没有足够的空间存储它时，淘汰最近最少访问**RDD**的一个分区。这样就保持旧的分区在内存中以防止来自同一个RDD的分区循环的进出内存。其中重要性在于大多数操作都是在整个RDD上运行任务，因此已经在内存中的分区很可能将来还会被用到。
#### 检查点支持
尽管在RDDs失效后总是可以使用沿袭来恢复，但是对于具有长沿袭链的RDDs这种恢复可能很耗时。因此对一些RDDs在稳定存储上做检查点是很有用的。通常检查点用在包含宽依赖的长沿袭图谱的RDDs上。在这种情况下，集群中一个节点的失效会导致丢失来自每个父RDD的数据分片，从而必须全部重新计算。相反，对于具有窄依赖的RDDs是不值得做检查点的。Spark当前提供了做检查点的API(一个对*persist*的REPLICATE标记)，由用户决定对哪个数据做检查点。最后，值得注意的是RDDs的只读特性使得做检查点要比通常的共享内存更加简单。因为不用关心一致性问题，RDDs能在后台被写出，不需要程序暂停或分布式快照方案。
